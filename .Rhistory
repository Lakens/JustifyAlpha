resplot2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
plot_data <- rbind(resplot1$plot_data, resplot2$plot_data, resplot3$plot_data)
plot_data$n <- as.factor(rep(c(10, 50, 100), each = 9999))
w_c_alpha_plot <- ggplot(data=plot_data, aes(x=alpha_list, y=w_c_list)) +
geom_line(size = 1.3, aes(linetype = n)) +
geom_point(aes(x = resplot1$alpha, y = (1 * resplot1$alpha + 1 * (resplot1$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot2$alpha, y = (1 * resplot2$alpha + 1 * (resplot2$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot3$alpha, y = (1 * resplot3$alpha + 1 * (resplot3$beta)) / (1 + 1)), color="red", size = 3) +
theme_minimal(base_size = 16) +
scale_x_continuous("alpha", seq(0,1,0.1)) +
scale_y_continuous("weighted combined error rate", seq(0,1,0.1), limits = c(0,1))
w_c_alpha_plot
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2.
res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
res3$alpha
res3$beta
res3$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_3 <- (costT1T2 * res3$alpha + priorH1H0 * res3$beta) / (priorH1H0 + costT1T2)
w_c_3
res4 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, printplot = TRUE)
res4$alpha
res4$beta
res4$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_4 <- (costT1T2 * res4$alpha + priorH1H0 * res4$beta) / (priorH1H0 + costT1T2)
# If we weigh Type 1 errors 4 times more, we do not just add the two errors # and divide by 2, but we multiple Type 1 errors by 4 and divide by 5.
# We divide by 5 to keep the combined weighed error rate between 0 and 1.
# For example, if both errors are 1, we have (1 * 4 + 1)/5 = 1
# note this ignores differences in priors.
(res4$alpha * 4 + res4$beta)/(5)
# This is equivalent to:
(res4$alpha + res4$beta/4)/(1.25)
# This is just the formula above but filled in and ignoring priors.
# This is the value you see in the curve as weighed combined error rate.
# If the cost of an error is 10000, the total weighed cost is:
w_c_4 * 10000
( (0.5 * 4 * res4$alpha * 10000) + (0.5 * res4$beta * 10000) )
# Replicate res4
a <- res4$alpha
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# increasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha + 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# decreasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha - 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# Thus, this is the optimal alpha to minimize costs.
res5 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, priorH1H0 = 0.11111)
res5$alpha
res5$beta
res5$error
res6 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, priorH1H0 = 0.11111, printplot = TRUE)
res6$alpha
res6$beta
res6$errorrate
costT1T2 = 1
priorH1H0 = 2
w <- (costT1T2 * res6$alpha + priorH1H0 * (res6$beta)) / (priorH1H0 + costT1T2)
res7 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = sample_n, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
# # Result is identical to:
# pow_res <- ceiling(pwr::pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.95, type = 'two.sample', alternative = 'two.sided')$n)
# res8 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = i, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", errorgoal = 0.05, error = "minimize", costT1T2 = 1, priorH1H0 = 0.1)
#
# restest <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 65, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 1, priorH1H0 = 0.1)
#
#  restest2 <- optimal_sample(power_function = "pwr::pwr.t.test(d = 0.5, n = sample_n, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", errorgoal = 0.05, error = "balance", costT1T2 = 1, priorH1H0 = 1)
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))
colors <- c("Beta" = "grey", "Weighted Combined Error Rate of 5%" = "red", "Weighted Combined Error Rate" = "black", "Alpha" = "grey")
linetypes <- c("Beta" = "dotted", "Weighted Combined Error Rate of 5%" = "solid", "Weighted Combined Error Rate" = "solid", "Alpha" = "dashed")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = Error, color = "Weighted Combined Error Rate", linetype = "Weighted Combined Error Rate")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Sample Size", seq(10,150,20)) +
scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) +
geom_line(aes(y=alphas,  color = "Alpha", linetype = "Alpha"), data=data, size = 1.3) +
geom_line(aes(y=betas,  color = "Beta", linetype = "Beta"), data=data, size = 1.3) +
geom_point(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, color = "Weighted Combined Error Rate of 5%", linetype = "Weighted Combined Error Rate of 5%"), size = 3) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
plot <- plot +   geom_segment(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], xend= data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, yend = -Inf), color = "red", linetype = "dotted")
plot
n1 <- 150
loops <- seq(from = 0, to = 7, by = 0.001)
p <- numeric(length(loops))
bf <- numeric(length(loops))
#d <- numeric(length(loops))
tval <- numeric(length(loops))
i <- 0
for(t in loops){
i <- i+1
bf[i] <- exp(BayesFactor::ttest.tstat(t, n1, rscale = 0.707, nullInterval = c(0, Inf))$bf)
p[i] <- pt(t, ((n1) - 1), lower=FALSE)
tval[i] <- t
#d[i] <- t * sqrt((1/n1)+(1/n2))
}
n1 <- 150
loops <- seq(from = 0, to = 7, by = 0.01)
p <- numeric(length(loops))
bf <- numeric(length(loops))
#d <- numeric(length(loops))
tval <- numeric(length(loops))
i <- 0
for(t in loops){
i <- i+1
bf[i] <- exp(BayesFactor::ttest.tstat(t, n1, rscale = 0.707, nullInterval = c(0, Inf))$bf)
p[i] <- pt(t, ((n1) - 1), lower=FALSE)
tval[i] <- t
#d[i] <- t * sqrt((1/n1)+(1/n2))
}
lindley  <- ttestEvidence(1,  150, one.sided = TRUE, rscale = 0.707)
moderate <- ttestEvidence(3,  150, one.sided = TRUE, rscale = 0.707)
strong   <- ttestEvidence(10, 150, one.sided = TRUE, rscale = 0.707)
plot(p, bf, type="l", lty=1, lwd=3, xlim = c(0, 0.05), ylim = c(0, 10), axes = F, xlab = "p-value", ylab = "Bayes factor")
axis(side=1, at = c(0, as.numeric(lindley), as.numeric(moderate), as.numeric(strong), 0.05), labels = c(0, round(lindley, digits = 3), round(moderate, digits = 3), round(strong, digits = 3), 0.05),  lwd = 3, las = 3)
axis(side=2, at = c(0.33, 1, 3, 10), labels = c("1/3", 1, 3, 10), lwd = 3)
abline(h = c(1, 3, 10), col = "gray", lty = 2)
abline(v = c(lindley, moderate, strong), lty = 3)
data <- read.csv("plotlindleydata.csv")
colors <- c("within-sample" = "grey", "between-sample" = "black")
linetypes <- c("within-sample" = "dotted", "between-sample" = "solid")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = cauchywithin, color = "within-sample", linetype = "within-sample")) +
geom_line(size = 1.3, aes(y = cauchybetween, color = "between-sample", linetype = "between-sample")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Total Sample Size (N)", seq(10,300,20)) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
lindley  <- ttestEvidence(1,  150, one.sided = TRUE, rscale = 0.707)
moderate <- ttestEvidence(3,  150, one.sided = TRUE, rscale = 0.707)
strong   <- ttestEvidence(10, 150, one.sided = TRUE, rscale = 0.707)
lindley
if(!require(justifieR)){devtools::install_github("Lakens/justifieR")}
library(justifieR)
library(ggplot2)
library(pwr)
library(papaja)
justifieR::ttestEvidence(2, 30)
devtools::install_github("Lakens/justifieR")
ttestEvidence(2, 20)
library(JustifyAlpha)
ttestEvidence(2, 20)
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2.
res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
res3$alpha
res3$beta
res3$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_3 <- (costT1T2 * res3$alpha + priorH1H0 * res3$beta) / (priorH1H0 + costT1T2)
w_c_3
res4 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, printplot = TRUE)
res4$alpha
res4$beta
res4$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_4 <- (costT1T2 * res4$alpha + priorH1H0 * res4$beta) / (priorH1H0 + costT1T2)
# If we weigh Type 1 errors 4 times more, we do not just add the two errors # and divide by 2, but we multiple Type 1 errors by 4 and divide by 5.
# We divide by 5 to keep the combined weighed error rate between 0 and 1.
# For example, if both errors are 1, we have (1 * 4 + 1)/5 = 1
# note this ignores differences in priors.
(res4$alpha * 4 + res4$beta)/(5)
# This is equivalent to:
(res4$alpha + res4$beta/4)/(1.25)
# This is just the formula above but filled in and ignoring priors.
# This is the value you see in the curve as weighed combined error rate.
# If the cost of an error is 10000, the total weighed cost is:
w_c_4 * 10000
( (0.5 * 4 * res4$alpha * 10000) + (0.5 * res4$beta * 10000) )
# Replicate res4
a <- res4$alpha
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# increasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha + 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# decreasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha - 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# Thus, this is the optimal alpha to minimize costs.
res4$plot
resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
library(JustifyAlpha)
resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
plot_data <- rbind(resplot1$plot_data, resplot2$plot_data, resplot3$plot_data)
plot_data$n <- as.factor(rep(c(10, 50, 100), each = 9999))
w_c_alpha_plot <- ggplot(data=plot_data, aes(x=alpha_list, y=w_c_list)) +
geom_line(size = 1.3, aes(linetype = n)) +
geom_point(aes(x = resplot1$alpha, y = (1 * resplot1$alpha + 1 * (resplot1$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot2$alpha, y = (1 * resplot2$alpha + 1 * (resplot2$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot3$alpha, y = (1 * resplot3$alpha + 1 * (resplot3$beta)) / (1 + 1)), color="red", size = 3) +
theme_minimal(base_size = 16) +
scale_x_continuous("alpha", seq(0,1,0.1)) +
scale_y_continuous("weighted combined error rate", seq(0,1,0.1), limits = c(0,1))
if(!require(JustifyAlpha)){devtools::install_github("Lakens/justifieR")}
library(JustifyAlpha)
library(ggplot2)
library(pwr)
library(papaja)
resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
plot_data <- rbind(resplot1$plot_data, resplot2$plot_data, resplot3$plot_data)
plot_data$n <- as.factor(rep(c(10, 50, 100), each = 9999))
w_c_alpha_plot <- ggplot(data=plot_data, aes(x=alpha_list, y=w_c_list)) +
geom_line(size = 1.3, aes(linetype = n)) +
geom_point(aes(x = resplot1$alpha, y = (1 * resplot1$alpha + 1 * (resplot1$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot2$alpha, y = (1 * resplot2$alpha + 1 * (resplot2$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot3$alpha, y = (1 * resplot3$alpha + 1 * (resplot3$beta)) / (1 + 1)), color="red", size = 3) +
theme_minimal(base_size = 16) +
scale_x_continuous("alpha", seq(0,1,0.1)) +
scale_y_continuous("weighted combined error rate", seq(0,1,0.1), limits = c(0,1))
w_c_alpha_plot
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2.
res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
library(JustifyAlpha)
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2.
res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
res3$alpha
res3$beta
res3$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_3 <- (costT1T2 * res3$alpha + priorH1H0 * res3$beta) / (priorH1H0 + costT1T2)
w_c_3
res4 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, printplot = TRUE)
res4$alpha
res4$beta
res4$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_4 <- (costT1T2 * res4$alpha + priorH1H0 * res4$beta) / (priorH1H0 + costT1T2)
# If we weigh Type 1 errors 4 times more, we do not just add the two errors # and divide by 2, but we multiple Type 1 errors by 4 and divide by 5.
# We divide by 5 to keep the combined weighed error rate between 0 and 1.
# For example, if both errors are 1, we have (1 * 4 + 1)/5 = 1
# note this ignores differences in priors.
(res4$alpha * 4 + res4$beta)/(5)
# This is equivalent to:
(res4$alpha + res4$beta/4)/(1.25)
# This is just the formula above but filled in and ignoring priors.
# This is the value you see in the curve as weighed combined error rate.
# If the cost of an error is 10000, the total weighed cost is:
w_c_4 * 10000
( (0.5 * 4 * res4$alpha * 10000) + (0.5 * res4$beta * 10000) )
# Replicate res4
a <- res4$alpha
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# increasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha + 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# decreasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha - 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# Thus, this is the optimal alpha to minimize costs.
install.pacakges("tinylabels")
install.packages("tinylabels")
plot
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))
colors <- c("Beta" = "grey", "Weighted Combined Error Rate of 5%" = "red", "Weighted Combined Error Rate" = "black", "Alpha" = "grey")
linetypes <- c("Beta" = "dotted", "Weighted Combined Error Rate of 5%" = "solid", "Weighted Combined Error Rate" = "solid", "Alpha" = "dashed")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = Error, color = "Weighted Combined Error Rate", linetype = "Weighted Combined Error Rate")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Sample Size", seq(10,150,20)) +
scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) +
geom_line(aes(y=alphas,  color = "Alpha", linetype = "Alpha"), data=data, size = 1.3) +
geom_line(aes(y=betas,  color = "Beta", linetype = "Beta"), data=data, size = 1.3) +
geom_point(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, color = "Weighted Combined Error Rate of 5%", linetype = "Weighted Combined Error Rate of 5%"), size = 3) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
plot <- plot +   geom_segment(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], xend= data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, yend = -Inf), color = "red", linetype = "dotted")
plot
if(!require(JustifyAlpha)){devtools::install_github("Lakens/justifieR")}
library(JustifyAlpha)
library(ggplot2)
library(pwr)
library(papaja)
library(tinylabels)
N <- 150
d <- 0.5
p <- 0.05
p_upper <- 0.05 + 0.00000001
p_lower <- 0.00000001
ymax <- 10 # Maximum value y-scale (only for p-curve)
# Calculations
se <- sqrt(2 / N) # standard error
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
# p-value function
pdf2_t <- function(p) 0.5 * dt(qt(p / 2, 2 * N - 2, 0), 2 * N - 2, ncp) / dt(qt(p / 2, 2 * N - 2, 0), 2 * N - 2, 0) + dt(qt(1 - p / 2, 2 * N - 2, 0), 2 * N - 2, ncp) / dt(qt(1 - p / 2, 2 * N - 2, 0), 2 * N - 2, 0)
plot(-10,
xlab = "P-value", ylab = "", axes = FALSE,
main = "P-value distribution for d = 0.5 and N = 150", xlim = c(0, .1), ylim = c(0, ymax), cex.lab = 1.5, cex.main = 1.5, cex.sub = 1.5
)
axis(side = 1, at = seq(0, 1, 0.01), labels = seq(0, 1, 0.01), cex.axis = 1.5)
# cord.x <- c(p_lower,seq(p_lower,p_upper,0.001),p_upper)
# cord.y <- c(0,pdf2_t(seq(p_lower, p_upper, 0.001)),0)
# polygon(cord.x,cord.y,col=rgb(0.5, 0.5, 0.5,0.5))
curve(pdf2_t, 0, .1, n = 1000, col = "black", lwd = 3, add = TRUE)
ncp <- (0 * sqrt(N / 2)) # Calculate non-centrality parameter d
curve(pdf2_t, 0, 1, n = 1000, col = "black", lty = 2, lwd = 3, add = TRUE)
abline(v = 0.05, col = "black", lty = 3, lwd = 3)
res1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1$alpha
res1$beta
res2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance")
res2$alpha
res2$beta
res1_n_10 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1_n_10$alpha
res1_n_10$beta
res1_n_100 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power")
res1_n_100$alpha
res1_n_100$beta
resplot1 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 10, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot2 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 50, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
resplot3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 100, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 1, printplot = FALSE)
plot_data <- rbind(resplot1$plot_data, resplot2$plot_data, resplot3$plot_data)
plot_data$n <- as.factor(rep(c(10, 50, 100), each = 9999))
w_c_alpha_plot <- ggplot(data=plot_data, aes(x=alpha_list, y=w_c_list)) +
geom_line(size = 1.3, aes(linetype = n)) +
geom_point(aes(x = resplot1$alpha, y = (1 * resplot1$alpha + 1 * (resplot1$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot2$alpha, y = (1 * resplot2$alpha + 1 * (resplot2$beta)) / (1 + 1)), color="red", size = 3) +
geom_point(aes(x = resplot3$alpha, y = (1 * resplot3$alpha + 1 * (resplot3$beta)) / (1 + 1)), color="red", size = 3) +
theme_minimal(base_size = 16) +
scale_x_continuous("alpha", seq(0,1,0.1)) +
scale_y_continuous("weighted combined error rate", seq(0,1,0.1), limits = c(0,1))
w_c_alpha_plot
#Cohen would weigh Type 1 four times as much as Type 2 errors
#We set n to 50 participants and d = 0.5, and costT1T2 to 4.
#We indeed see the recommended alpha is 0.05 and beta is 0.2.
res3 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, printplot = TRUE)
res3$alpha
res3$beta
res3$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_3 <- (costT1T2 * res3$alpha + priorH1H0 * res3$beta) / (priorH1H0 + costT1T2)
w_c_3
res4 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "minimize", costT1T2 = 4, printplot = TRUE)
res4$alpha
res4$beta
res4$errorrate
costT1T2 = 4
priorH1H0 = 1
w_c_4 <- (costT1T2 * res4$alpha + priorH1H0 * res4$beta) / (priorH1H0 + costT1T2)
# If we weigh Type 1 errors 4 times more, we do not just add the two errors # and divide by 2, but we multiple Type 1 errors by 4 and divide by 5.
# We divide by 5 to keep the combined weighed error rate between 0 and 1.
# For example, if both errors are 1, we have (1 * 4 + 1)/5 = 1
# note this ignores differences in priors.
(res4$alpha * 4 + res4$beta)/(5)
# This is equivalent to:
(res4$alpha + res4$beta/4)/(1.25)
# This is just the formula above but filled in and ignoring priors.
# This is the value you see in the curve as weighed combined error rate.
# If the cost of an error is 10000, the total weighed cost is:
w_c_4 * 10000
( (0.5 * 4 * res4$alpha * 10000) + (0.5 * res4$beta * 10000) )
# Replicate res4
a <- res4$alpha
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# increasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha + 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# decreasing alpha by 0.001 for analysis 4 increases costs
a <- res4$alpha - 0.001
b <- 1 - pwr::pwr.t.test(d = 0.5, n = 50, sig.level = a, type = 'two.sample', alternative = 'two.sided')$power
( (0.5 * 4 * a * 10000) + (0.5 * b * 10000) )
# Thus, this is the optimal alpha to minimize costs.
res4$plot
res5 <- optimal_alpha(power_function = "pwr::pwr.t.test(d = 0.5, n = 64, sig.level = x, type = 'two.sample', alternative = 'two.sided')$power", error = "balance", costT1T2 = 4, priorH1H0 = 0.11111)
plot
plot
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))
colors <- c("Beta" = "grey", "Weighted Combined Error Rate of 5%" = "red", "Weighted Combined Error Rate" = "black", "Alpha" = "grey")
linetypes <- c("Beta" = "dotted", "Weighted Combined Error Rate of 5%" = "solid", "Weighted Combined Error Rate" = "solid", "Alpha" = "dashed")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = Error, color = "Weighted Combined Error Rate", linetype = "Weighted Combined Error Rate")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Sample Size", seq(10,150,20)) +
scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) +
geom_line(aes(y=alphas,  color = "Alpha", linetype = "Alpha"), data=data, size = 1.3) +
geom_line(aes(y=betas,  color = "Beta", linetype = "Beta"), data=data, size = 1.3) +
geom_point(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, color = "Weighted Combined Error Rate of 5%", linetype = "Weighted Combined Error Rate of 5%"), size = 3) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
plot <- plot +   geom_segment(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], xend= data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, yend = -Inf), color = "red", linetype = "dotted")
plot
plot
n1 <- 150
loops <- seq(from = 0, to = 7, by = 0.01)
p <- numeric(length(loops))
bf <- numeric(length(loops))
#d <- numeric(length(loops))
tval <- numeric(length(loops))
i <- 0
for(t in loops){
i <- i+1
bf[i] <- exp(BayesFactor::ttest.tstat(t, n1, rscale = 0.707, nullInterval = c(0, Inf))$bf)
p[i] <- pt(t, ((n1) - 1), lower=FALSE)
tval[i] <- t
#d[i] <- t * sqrt((1/n1)+(1/n2))
}
plot
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))
colors <- c("Beta" = "grey", "Weighted Combined Error Rate of 5%" = "red", "Weighted Combined Error Rate" = "black", "Alpha" = "grey")
linetypes <- c("Beta" = "dotted", "Weighted Combined Error Rate of 5%" = "solid", "Weighted Combined Error Rate" = "solid", "Alpha" = "dashed")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = Error, color = "Weighted Combined Error Rate", linetype = "Weighted Combined Error Rate")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Sample Size", seq(10,150,20)) +
scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) +
geom_line(aes(y=alphas,  color = "Alpha", linetype = "Alpha"), data=data, size = 1.3) +
geom_line(aes(y=betas,  color = "Beta", linetype = "Beta"), data=data, size = 1.3) +
geom_point(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, color = "Weighted Combined Error Rate of 5%", linetype = "Weighted Combined Error Rate of 5%"), size = 3) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
plot <- plot +   geom_segment(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], xend= data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, yend = -Inf), color = "red", linetype = "dotted")
data <- read.csv("plotdata.csv")
samplesize <- 10:150
line <- rep(0.05, length(samplesize))
data <- as.data.frame(cbind(samplesize,line, data))
colors <- c("Beta" = "grey", "Weighted Combined Error Rate of 5%" = "red", "Weighted Combined Error Rate" = "black", "Alpha" = "grey")
linetypes <- c("Beta" = "dotted", "Weighted Combined Error Rate of 5%" = "solid", "Weighted Combined Error Rate" = "solid", "Alpha" = "dashed")
library(ggplot2)
plot <- ggplot(aes(x=samplesize), data=data) +
geom_line(size = 1.3, aes(y = Error, color = "Weighted Combined Error Rate", linetype = "Weighted Combined Error Rate")) +
theme_minimal(base_size = 18) +
scale_x_continuous("Sample Size", seq(10,150,20)) +
scale_y_continuous("",seq(0,1,0.05), limits = c(0,0.5)) +
geom_line(aes(y=alphas,  color = "Alpha", linetype = "Alpha"), data=data, size = 1.3) +
geom_line(aes(y=betas,  color = "Beta", linetype = "Beta"), data=data, size = 1.3) +
geom_point(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, color = "Weighted Combined Error Rate of 5%", linetype = "Weighted Combined Error Rate of 5%"), size = 3) +
labs(x = "Sample Size",
y = "",
color = "",
linetype = "") +
scale_color_manual(values = colors) +
scale_linetype_manual(values = linetypes) +
theme(legend.position="bottom")
plot <- plot +   geom_segment(aes(x = data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], xend= data$samplesize[data$Error == max(data$Error[data$Error < 0.05])], y = 0.05, yend = -Inf), color = "red", linetype = "dotted")
devtools::install_github("crsh/papaja")
install.packages("pkgdown")
pkgdown::build_site()
setwd("C:/Users/Maximilian Maier/Desktop/PaperWriting/IdealAlpha/JustifieR")
pkgdown::build_site()
